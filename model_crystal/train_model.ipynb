{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557\n",
      "Epoch 1/50\n",
      "369/369 [==============================] - 226s 597ms/step - loss: 2.6455 - accuracy: 0.1129 - val_loss: 2.4237 - val_accuracy: 0.1786\n",
      "Epoch 2/50\n",
      "369/369 [==============================] - 222s 601ms/step - loss: 2.2764 - accuracy: 0.2165 - val_loss: 2.4169 - val_accuracy: 0.1847\n",
      "Epoch 3/50\n",
      "369/369 [==============================] - 223s 606ms/step - loss: 1.9558 - accuracy: 0.3232 - val_loss: 2.4452 - val_accuracy: 0.2206\n",
      "Epoch 4/50\n",
      "369/369 [==============================] - 222s 602ms/step - loss: 1.5991 - accuracy: 0.4580 - val_loss: 2.5579 - val_accuracy: 0.2115\n",
      "Epoch 5/50\n",
      "369/369 [==============================] - 266s 722ms/step - loss: 1.2264 - accuracy: 0.6023 - val_loss: 2.7561 - val_accuracy: 0.1947\n",
      "Epoch 6/50\n",
      "369/369 [==============================] - 251s 680ms/step - loss: 0.8204 - accuracy: 0.7386 - val_loss: 3.1094 - val_accuracy: 0.1756\n",
      "Epoch 7/50\n",
      "369/369 [==============================] - 240s 650ms/step - loss: 0.5558 - accuracy: 0.8314 - val_loss: 3.4395 - val_accuracy: 0.1962\n",
      "Epoch 8/50\n",
      "369/369 [==============================] - 253s 685ms/step - loss: 0.4085 - accuracy: 0.8770 - val_loss: 3.6583 - val_accuracy: 0.1870\n",
      "Epoch 9/50\n",
      "369/369 [==============================] - 238s 645ms/step - loss: 0.3060 - accuracy: 0.9084 - val_loss: 3.9875 - val_accuracy: 0.1756\n",
      "Epoch 10/50\n",
      "369/369 [==============================] - 222s 601ms/step - loss: 0.2344 - accuracy: 0.9320 - val_loss: 4.2227 - val_accuracy: 0.1702\n",
      "Epoch 11/50\n",
      "369/369 [==============================] - 214s 579ms/step - loss: 0.1938 - accuracy: 0.9436 - val_loss: 4.4055 - val_accuracy: 0.1855\n",
      "Epoch 12/50\n",
      "369/369 [==============================] - 215s 583ms/step - loss: 0.1773 - accuracy: 0.9476 - val_loss: 4.6301 - val_accuracy: 0.1779\n",
      "Epoch 13/50\n",
      "369/369 [==============================] - 214s 579ms/step - loss: 0.2166 - accuracy: 0.9373 - val_loss: 4.2045 - val_accuracy: 0.1489\n",
      "Epoch 14/50\n",
      "369/369 [==============================] - 211s 573ms/step - loss: 0.2206 - accuracy: 0.9347 - val_loss: 4.8586 - val_accuracy: 0.1473\n",
      "Epoch 15/50\n",
      "369/369 [==============================] - 212s 574ms/step - loss: 0.1705 - accuracy: 0.9494 - val_loss: 4.9816 - val_accuracy: 0.1489\n",
      "Epoch 16/50\n",
      "369/369 [==============================] - 211s 571ms/step - loss: 0.1250 - accuracy: 0.9624 - val_loss: 5.1833 - val_accuracy: 0.1504\n",
      "Epoch 17/50\n",
      "369/369 [==============================] - 211s 572ms/step - loss: 0.1045 - accuracy: 0.9674 - val_loss: 5.4810 - val_accuracy: 0.1595\n",
      "Epoch 18/50\n",
      "369/369 [==============================] - 212s 573ms/step - loss: 0.1190 - accuracy: 0.9638 - val_loss: 5.4369 - val_accuracy: 0.1580\n",
      "Epoch 19/50\n",
      "369/369 [==============================] - 212s 574ms/step - loss: 0.1453 - accuracy: 0.9559 - val_loss: 5.4305 - val_accuracy: 0.1489\n",
      "Epoch 20/50\n",
      "369/369 [==============================] - 213s 578ms/step - loss: 0.1412 - accuracy: 0.9575 - val_loss: 5.1831 - val_accuracy: 0.1542\n",
      "Epoch 21/50\n",
      "369/369 [==============================] - 215s 584ms/step - loss: 0.1168 - accuracy: 0.9637 - val_loss: 5.4369 - val_accuracy: 0.1588\n",
      "Epoch 22/50\n",
      "369/369 [==============================] - 215s 581ms/step - loss: 0.1249 - accuracy: 0.9616 - val_loss: 5.1514 - val_accuracy: 0.1588\n",
      "Epoch 23/50\n",
      "369/369 [==============================] - 211s 573ms/step - loss: 0.1159 - accuracy: 0.9634 - val_loss: 5.4901 - val_accuracy: 0.1641\n",
      "Epoch 24/50\n",
      "369/369 [==============================] - 215s 582ms/step - loss: 0.1042 - accuracy: 0.9678 - val_loss: 5.5942 - val_accuracy: 0.1588\n",
      "Epoch 25/50\n",
      "369/369 [==============================] - 211s 573ms/step - loss: 0.1037 - accuracy: 0.9672 - val_loss: 5.6723 - val_accuracy: 0.1718\n",
      "Epoch 26/50\n",
      "369/369 [==============================] - 212s 575ms/step - loss: 0.0898 - accuracy: 0.9716 - val_loss: 5.8876 - val_accuracy: 0.1710\n",
      "Epoch 27/50\n",
      "144/369 [==========>...................] - ETA: 6:46 - loss: 0.0883 - accuracy: 0.9720"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     69\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_text, y_train, validation_data\u001b[39m=\u001b[39;49m(X_test_text, y_test),\n\u001b[0;32m     71\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[0;32m     73\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mmodel2_with_13000.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\fayyo\\.conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('valuable_data_lastone.csv')\n",
    "cols=['position 1', 'position 2', 'field of studies 1','experince 1', 'experince 2', 'field of studies 2',\n",
    "       'degree 1', 'degree 2', 'industry', 'skills', 'influencer', 'country']\n",
    "\n",
    "for col in cols:\n",
    "  data[col] = data[col].fillna(\"Unknown\")\n",
    "# Convert non-string values to strings in the feature columns\n",
    "text_features = data[['position 1', 'position 2', \"experince 1\",\"experince 2\", \n",
    "                      'field of studies 1', 'field of studies 2', \n",
    "                      'degree 1', 'degree 2', 'industry', 'skills',\n",
    "                      'influencer', 'country', 'summary']].copy()\n",
    "\n",
    "# Handle non-string values in each column\n",
    "for column in text_features.columns:\n",
    "    text_features[column] = text_features[column].astype(str)\n",
    "\n",
    "# Combine all text features into a single string column\n",
    "text_data = text_features.apply(lambda x: ' '.join(x), axis=1).tolist()\n",
    "labels = data['characters'].tolist()\n",
    "\n",
    "# Convert labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(set(labels))}\n",
    "y = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Update label mapping to start from 0\n",
    "label_mapping = {label: idx for label, idx in label_mapping.items()}\n",
    "num_classes = len(label_mapping)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to have consistent length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "print(max_sequence_length)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert the data to NumPy arrays\n",
    "X_text = np.array(padded_sequences)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Textual input branch\n",
    "text_input = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(vocab_size, 100, input_length=max_sequence_length)(text_input)\n",
    "lstm_layer = LSTM(100)(embedding_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(lstm_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=text_input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_text, y_train, validation_data=(X_test_text, y_test),\n",
    "          epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model\n",
    "model.save('model2_with_13000.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"./last_60k_with_chars.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "characters\n",
       "Cs (Editor)        5810\n",
       "C (Analyst)        4490\n",
       "SC (Stabilizer)    4043\n",
       "Sc (Planner)       3415\n",
       "S (Supporter)      3009\n",
       "Cd (Skeptic)       2885\n",
       "Si (Counselor)     2883\n",
       "Is (Encourager)    2858\n",
       "IS (Harmonizer)    2808\n",
       "I (Motivator)      2424\n",
       "Id (Influencer)    2193\n",
       "CD (Questioner)    1977\n",
       "DI (Initiator)     1752\n",
       "Di (Driver)        1749\n",
       "Dc (Architect)     1689\n",
       "D (Captain)        1516\n",
       "Encourager (Is)    1481\n",
       "Harmonizer (IS)    1467\n",
       "Editor (Cs)        1383\n",
       "Counselor (Si)     1331\n",
       "Analyst (C)        1153\n",
       "Supporter (S)      1130\n",
       "Motivator (I)      1126\n",
       "Planner (Sc)       1120\n",
       "Stabilizer (SC)    1075\n",
       "Influencer (Id)     901\n",
       "Skeptic (Cd)        838\n",
       "Driver (Di)         779\n",
       "Initiator (DI)      746\n",
       "Questioner (CD)     634\n",
       "Captain (D)         622\n",
       "Architect (Dc)      610\n",
       "Driver (Di);          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['characters'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"last_60k_with_chars.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_element(value):\n",
    "    # Example: Multiply each element by 2\n",
    "    a=value.replace(\" \",\"\").replace(\")\",\"\").split(\"(\")\n",
    "    return max(a, key=len)\n",
    "\n",
    "# Use a for loop to iterate through the column and modify elements\n",
    "for index, row in data.iterrows():\n",
    "    data.at[index, 'characters'] = modify_element(row['characters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "characters\n",
       "Editor        7193\n",
       "Analyst       5643\n",
       "Stabilizer    5118\n",
       "Planner       4535\n",
       "Encourager    4339\n",
       "Harmonizer    4275\n",
       "Counselor     4214\n",
       "Supporter     4139\n",
       "Skeptic       3723\n",
       "Motivator     3550\n",
       "Influencer    3094\n",
       "Questioner    2611\n",
       "Driver        2529\n",
       "Initiator     2498\n",
       "Architect     2299\n",
       "Captain       2138\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['characters'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24935379644588046\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Analyst       0.31      0.25      0.28      1142\n",
      "   Architect       0.19      0.16      0.17       437\n",
      "     Captain       0.30      0.10      0.15       404\n",
      "   Counselor       0.21      0.29      0.24       816\n",
      "      Driver       0.27      0.27      0.27       539\n",
      "      Editor       0.33      0.54      0.41      1395\n",
      "  Encourager       0.20      0.33      0.25       862\n",
      "  Harmonizer       0.24      0.19      0.21       877\n",
      "  Influencer       0.21      0.22      0.22       636\n",
      "   Initiator       0.24      0.18      0.21       485\n",
      "   Motivator       0.13      0.05      0.07       683\n",
      "     Planner       0.23      0.24      0.23       905\n",
      "  Questioner       0.21      0.08      0.12       547\n",
      "     Skeptic       0.22      0.27      0.24       760\n",
      "  Stabilizer       0.31      0.25      0.27      1048\n",
      "   Supporter       0.18      0.15      0.16       844\n",
      "\n",
      "    accuracy                           0.25     12380\n",
      "   macro avg       0.24      0.22      0.22     12380\n",
      "weighted avg       0.24      0.25      0.24     12380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "X = data[['position 1', 'position 2', 'field of studies 1', 'field of studies 2',\n",
    "          'degree 1', 'degree 2', 'industry', 'skills',  'summary']]\n",
    "y = data['characters']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Preprocess text data using spaCy for tokenization:\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc])\n",
    "\n",
    "X_train['tokenized_text'] = X_train.apply(lambda row: tokenize_text(\" \".join(row.astype(str))), axis=1)\n",
    "X_test['tokenized_text'] = X_test.apply(lambda row: tokenize_text(\" \".join(row.astype(str))), axis=1)\n",
    "\n",
    "# TF-IDF Vectorization:\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['tokenized_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['tokenized_text'])\n",
    "\n",
    "# Train a classification model (e.g., Multinomial Naive Bayes):\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set:\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance:\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digital Marketing Manager\n",
      "Predicted Character: Cd (Skeptic)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "positions0=[]\n",
    "positions1=[]\n",
    "\n",
    "skills=[]\n",
    "field_of_studies0=[]\n",
    "field_of_studies1=[]\n",
    "degrees0=[]\n",
    "degrees1=[]\n",
    "industries=[]\n",
    "summaries=[]\n",
    "# model_crystal\\datas\\13000_profiles_without_duplicates.csv\n",
    "with open(f'./datas/json_responses_13000/13075.json', 'r') as json_file:\n",
    "    res = json.load(json_file)\n",
    "        \n",
    "try:\n",
    "    skills.append(res['skills'])\n",
    "except:\n",
    "    skills.append(None)\n",
    "try:\n",
    "    position0=res['position_groups'][0]['profile_positions'][0]['title']\n",
    "    print(position0)\n",
    "    positions0.append(position0)\n",
    "except:\n",
    "    positions0.append(None)\n",
    "\n",
    "try:\n",
    "    position1=res['position_groups'][1]['profile_positions'][0]['title']\n",
    "    positions1.append(position1)\n",
    "except:\n",
    "    positions1.append(None)\n",
    "\n",
    "field_of_study0=\"\"\n",
    "try:\n",
    "    field_of_study0=res['education'][0]['field_of_study']\n",
    "    field_of_studies0.append(field_of_study0)\n",
    "except:\n",
    "    field_of_study0=None\n",
    "    field_of_studies0.append(field_of_study0)\n",
    "\n",
    "try:\n",
    "    field_of_study1=res['education'][1]['field_of_study']\n",
    "    field_of_studies1.append(field_of_study1)\n",
    "except:\n",
    "    field_of_study1=None\n",
    "    field_of_studies1.append(field_of_study1)\n",
    "\n",
    "try:\n",
    "    degree_name0=res['education'][0]['degree_name']\n",
    "    degrees0.append(degree_name0)\n",
    "except:\n",
    "    degree_name0=None\n",
    "    degrees0.append(degree_name0)\n",
    "\n",
    "try:\n",
    "    degree_name1=res['education'][1]['degree_name']\n",
    "    degrees1.append(degree_name1)\n",
    "except:\n",
    "    degree_name1=None\n",
    "    degrees1.append(degree_name1)\n",
    "\n",
    "try:\n",
    "    summaries.append(res['summary'])\n",
    "except:\n",
    "    summaries.append(None)\n",
    "try:\n",
    "    industries.append(res['industry'])\n",
    "except:\n",
    "    industries.append(None)\n",
    "\n",
    "user_input = pd.DataFrame({\n",
    "    'position 1': positions0[0],\n",
    "    'position 2': positions1[0],\n",
    "    'field of studies 1': field_of_studies0[0],\n",
    "\n",
    "    'field of studies 2': field_of_studies1[0],\n",
    "    'degree 1':degrees0[0],\n",
    "    'degree 2':degrees1[0],\n",
    "    'industry': industries[0],\n",
    "    'skills': skills[0],\n",
    "})\n",
    "user_input['tokenized_text'] = user_input.apply(lambda row: tokenize_text(\" \".join(row.astype(str))), axis=1)\n",
    "\n",
    "# TF-IDF Vectorization for user input\n",
    "user_input_tfidf = tfidf_vectorizer.transform(user_input['tokenized_text'])\n",
    "\n",
    "# Predict the character for the user input\n",
    "predicted_character = clf.predict(user_input_tfidf)\n",
    "\n",
    "print(\"Predicted Character:\", predicted_character[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "editor-stabilizer\n",
    "editor-editor\n",
    "analyst-editor\n",
    "Encourager-Encourager\n",
    "Initiator-Skeptic?\n",
    "Stabilizer-Stabilizer\n",
    "Driver-Architect\n",
    "Captain-Architect\n",
    "Encourager-Encourager\n",
    "Analyst-Stabilizer\n",
    "Initiator-Influencer\n",
    "Encourager-Supporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine_learning_1st_attempt_60k.joblib']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model_filename = 'machine_learning_1st_attempt_60k.joblib'\n",
    "joblib.dump(clf, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "loaded_model = joblib.load('machine_learning_1st_attempt.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26809369951534734\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.29      0.29      1142\n",
      "           1       0.25      0.21      0.23       437\n",
      "           2       0.24      0.17      0.20       404\n",
      "           3       0.23      0.26      0.25       816\n",
      "           4       0.26      0.23      0.25       539\n",
      "           5       0.35      0.48      0.40      1395\n",
      "           6       0.25      0.31      0.28       862\n",
      "           7       0.24      0.26      0.25       877\n",
      "           8       0.25      0.25      0.25       636\n",
      "           9       0.24      0.19      0.22       485\n",
      "          10       0.21      0.20      0.20       683\n",
      "          11       0.27      0.26      0.26       905\n",
      "          12       0.22      0.14      0.17       547\n",
      "          13       0.25      0.22      0.23       760\n",
      "          14       0.29      0.27      0.28      1048\n",
      "          15       0.26      0.21      0.23       844\n",
      "\n",
      "    accuracy                           0.27     12380\n",
      "   macro avg       0.26      0.25      0.25     12380\n",
      "weighted avg       0.26      0.27      0.26     12380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "data=pd.read_csv(\"./last_60k_with_chars.csv\")\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess text data using spaCy for tokenization\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc])\n",
    "X = data[['position 1', 'position 2', 'field of studies 1', 'field of studies 2',\n",
    "          'degree 1', 'degree 2', 'industry', 'skills',  'summary']]\n",
    "y = data['characters']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply label encoding to the target variable for both training and testing sets\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Tokenize and TF-IDF Vectorization\n",
    "X_train['tokenized_text'] = X_train.apply(lambda row: tokenize_text(\" \".join(row.astype(str))), axis=1)\n",
    "X_test['tokenized_text'] = X_test.apply(lambda row: tokenize_text(\" \".join(row.astype(str))), axis=1)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['tokenized_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['tokenized_text'])\n",
    "\n",
    "# Create and train an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>characters</th>\n",
       "      <th>position 1</th>\n",
       "      <th>position 2</th>\n",
       "      <th>field of studies 1</th>\n",
       "      <th>field of studies 2</th>\n",
       "      <th>degree 1</th>\n",
       "      <th>degree 2</th>\n",
       "      <th>industry</th>\n",
       "      <th>skills</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Encourager</td>\n",
       "      <td>Docent Nederlands</td>\n",
       "      <td>Auteur</td>\n",
       "      <td>Opleiding leraar Nederlands in de eerste graad</td>\n",
       "      <td>Nederlandse taal- en letterkunde</td>\n",
       "      <td>Master</td>\n",
       "      <td>Master of Arts - MA</td>\n",
       "      <td>Primary/Secondary Education</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Counselor</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>bestuurslid (secretaris)</td>\n",
       "      <td>Comparative Literature</td>\n",
       "      <td>English Language and Literature/Letters</td>\n",
       "      <td>Research master</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Information Technology &amp; Services</td>\n",
       "      <td>['Academic Writing', 'English Literature', 'Li...</td>\n",
       "      <td>Bij Infi ben ik verantwoordelijk voor het verz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Driver</td>\n",
       "      <td>Verkoopmedewerker</td>\n",
       "      <td>Bemonsteraar</td>\n",
       "      <td>Algemene literatuurwetenschap</td>\n",
       "      <td>Office Management</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>Associate's degree</td>\n",
       "      <td>Apparel &amp; Fashion</td>\n",
       "      <td>['Engels', 'Marketing', 'Communicatie', 'Analy...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Harmonizer</td>\n",
       "      <td>Directeur</td>\n",
       "      <td>Adviseur</td>\n",
       "      <td>International Relations and Affairs</td>\n",
       "      <td>Literature</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Performing Arts</td>\n",
       "      <td>['Journalism', 'Editing', 'Politics', 'Copywri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Counselor</td>\n",
       "      <td>Content marketeer</td>\n",
       "      <td>Interim team lead &amp; web content editor</td>\n",
       "      <td>Letterkunde - Literair Bedrijf</td>\n",
       "      <td>Algemene Cultuurwetenschappen</td>\n",
       "      <td>Master's Degree</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Internet</td>\n",
       "      <td>['Editing', 'Social Media', 'Dutch', 'Journali...</td>\n",
       "      <td>Creatieve contentmarketeer met een passie voor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  characters         position 1  \\\n",
       "0           0  Encourager  Docent Nederlands   \n",
       "1           1   Counselor         HR Manager   \n",
       "2           2      Driver  Verkoopmedewerker   \n",
       "3           3  Harmonizer          Directeur   \n",
       "4           4   Counselor  Content marketeer   \n",
       "\n",
       "                               position 2  \\\n",
       "0                                  Auteur   \n",
       "1                bestuurslid (secretaris)   \n",
       "2                            Bemonsteraar   \n",
       "3                                Adviseur   \n",
       "4  Interim team lead & web content editor   \n",
       "\n",
       "                               field of studies 1  \\\n",
       "0  Opleiding leraar Nederlands in de eerste graad   \n",
       "1                          Comparative Literature   \n",
       "2                   Algemene literatuurwetenschap   \n",
       "3             International Relations and Affairs   \n",
       "4                  Letterkunde - Literair Bedrijf   \n",
       "\n",
       "                        field of studies 2           degree 1  \\\n",
       "0         Nederlandse taal- en letterkunde             Master   \n",
       "1  English Language and Literature/Letters    Research master   \n",
       "2                        Office Management  Bachelor's degree   \n",
       "3                               Literature    Master's degree   \n",
       "4            Algemene Cultuurwetenschappen    Master's Degree   \n",
       "\n",
       "              degree 2                           industry  \\\n",
       "0  Master of Arts - MA        Primary/Secondary Education   \n",
       "1                  NaN  Information Technology & Services   \n",
       "2   Associate's degree                  Apparel & Fashion   \n",
       "3                  NaN                    Performing Arts   \n",
       "4    Bachelor's Degree                           Internet   \n",
       "\n",
       "                                              skills  \\\n",
       "0                                                 []   \n",
       "1  ['Academic Writing', 'English Literature', 'Li...   \n",
       "2  ['Engels', 'Marketing', 'Communicatie', 'Analy...   \n",
       "3  ['Journalism', 'Editing', 'Politics', 'Copywri...   \n",
       "4  ['Editing', 'Social Media', 'Dutch', 'Journali...   \n",
       "\n",
       "                                             summary  \n",
       "0                                                NaN  \n",
       "1  Bij Infi ben ik verantwoordelijk voor het verz...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  Creatieve contentmarketeer met een passie voor...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(tfidf_vectorizer, open(\"vectorizer.pickle\", \"wb\")) #//Save vectorizer\n",
    "pickle.load(open(\"vectorizer.pickle\", 'rb'))    #// Load vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XGBoost_1st_attempt_60k.joblib']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model_filename = 'XGBoost_1st_attempt_60k.joblib'\n",
    "joblib.dump(xgb_classifier, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=\"https://www.linkedin.com/in/korine-morgan-866542150/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/in/james-efmorfidis-0a4429209/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korine-morgan-866542150\n"
     ]
    }
   ],
   "source": [
    "id = link.split('/')[4]\n",
    "url = \"https://api.iscraper.io/v2/profile-details\"\n",
    "\n",
    "payload = {\n",
    "    'profile_id': id,\n",
    "}\n",
    "print(id)\n",
    "headers = {\n",
    "    'X-API-KEY': 'hVSqiv11cY1W5YUawXUDLBn0jb4G5W44',\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "res=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Criminologie'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['education'][1]['field_of_study']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "positions0=[]\n",
    "positions1=[]\n",
    "skills=[]\n",
    "field_of_studies0=[]\n",
    "field_of_studies1=[]\n",
    "degrees0=[]\n",
    "degrees1=[]\n",
    "industries=[]\n",
    "summaries=[]\n",
    "if res['skills']:\n",
    "    skills.append(res['skills'])\n",
    "else:\n",
    "    skills.append(\"NaN\")\n",
    "    print(\"HI\")\n",
    "if res['position_groups'][0]['profile_positions'][0]['title']:\n",
    "    position0=res['position_groups'][0]['profile_positions'][0]['title']\n",
    "    positions0.append(position0)\n",
    "else:\n",
    "    positions0.append(\"NaN\")\n",
    "try:\n",
    "    position1=res['position_groups'][1]['profile_positions'][0]['title']\n",
    "    positions1.append(position1)\n",
    "except:\n",
    "    positions1.append(\"NaN\")\n",
    "try:\n",
    "    field_of_study0=res['education'][0]['field_of_study']\n",
    "    field_of_studies0.append(field_of_study0)\n",
    "except:\n",
    "    field_of_studies0.append(\"NaN\")\n",
    "try:\n",
    "    field_of_study1=res['education'][1]['field_of_study']\n",
    "    field_of_studies1.append(field_of_study1)\n",
    "except:\n",
    "    field_of_studies1.append(\"NaN\")\n",
    "try:\n",
    "    degree_name0=res['education'][0]['degree_name']\n",
    "    degrees0.append(degree_name0)\n",
    "except:\n",
    "    degrees0.append(\"NaN\")\n",
    "try:\n",
    "    degree_name1=res['education'][1]['degree_name']\n",
    "    degrees1.append(degree_name1)\n",
    "except:\n",
    "    degrees1.append(\"NaN\")\n",
    "try:\n",
    "    summaries.append(res['summary'])\n",
    "except:\n",
    "    summaries.append(\"NaN\")\n",
    "try:\n",
    "    industries.append(res['industry'])\n",
    "except:\n",
    "    industries.append(\"NaN\")\n",
    "\n",
    "user_input = pd.DataFrame({\n",
    "    'position 1': positions0[0],\n",
    "    'position 2': positions1[0],\n",
    "    'field of studies 1': field_of_studies0[0],\n",
    "    'field of studies 2': field_of_studies1[0],\n",
    "    'degree 1':degrees0[0],\n",
    "    'degree 2':degrees1[0],\n",
    "    'industry': industries[0],\n",
    "    'skills': skills[0],\n",
    "}, index=[0])\n",
    "user_input['tokenized_text'] = user_input.apply(lambda row: tokenize_text(\" \".join(row.astype(str))), axis=1)\n",
    "\n",
    "# TF-IDF Vectorization for user input\n",
    "user_input_tfidf = tfidf_vectorizer.transform(user_input['tokenized_text'])\n",
    "\n",
    "# Predict the character for the user input\n",
    "predicted_character = xgb_classifier.predict(user_input_tfidf)\n",
    "\n",
    "# print(\"Predicted Character:\", label_encoder.inverse_transform(predicted_character)[0])\n",
    "# dict_[i]=predicted_character\n",
    "# dict_list.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position 1</th>\n",
       "      <th>position 2</th>\n",
       "      <th>field of studies 1</th>\n",
       "      <th>field of studies 2</th>\n",
       "      <th>degree 1</th>\n",
       "      <th>degree 2</th>\n",
       "      <th>industry</th>\n",
       "      <th>skills</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beleidsmedewerker</td>\n",
       "      <td>Credible Messenger</td>\n",
       "      <td>Sociale wetenschappen</td>\n",
       "      <td>Criminologie</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Research</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Beleidsmedewerker Credible Messenger Sociale w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          position 1          position 2     field of studies 1  \\\n",
       "0  Beleidsmedewerker  Credible Messenger  Sociale wetenschappen   \n",
       "\n",
       "  field of studies 2 degree 1 degree 2  industry skills  \\\n",
       "0       Criminologie     None     None  Research    NaN   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  Beleidsmedewerker Credible Messenger Sociale w...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beleidsmedewerker'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['position_groups'][0]['profile_positions'][0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position 1</th>\n",
       "      <th>position 2</th>\n",
       "      <th>field of studies 1</th>\n",
       "      <th>field of studies 2</th>\n",
       "      <th>degree 1</th>\n",
       "      <th>degree 2</th>\n",
       "      <th>industry</th>\n",
       "      <th>skills</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [position 1, position 2, field of studies 1, field of studies 2, degree 1, degree 2, industry, skills, tokenized_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'profile_id': 'james-efmorfidis-0a4429209',\n",
       " 'first_name': 'James',\n",
       " 'last_name': 'Efmorfidis',\n",
       " 'sub_title': 'Soccer Player at RKC Waalwijk',\n",
       " 'profile_picture': 'https://media.licdn.com/dms/image/C4E03AQEfuZCuGu5HiA/profile-displayphoto-shrink_800_800/0/1619700178644?e=1701907200&v=beta&t=p-51YNTEmeIqkQcCLwtww-fcHQFGTfJuVMB7-4Vp3JE',\n",
       " 'background_image': 'https://media.licdn.com/dms/image/C4E16AQGMQ5VXQDbprA/profile-displaybackgroundimage-shrink_350_1400/0/1619700248345?e=1701907200&v=beta&t=KPDBl1aJXqRpgaCyxXXnhh30QdkifblmkXXp5ZXSf6o',\n",
       " 'profile_type': 'personal',\n",
       " 'entity_urn': 'ACoAADTsHzwBJq0JGszXMZKLseDfSZlmtaE-GUM',\n",
       " 'object_urn': 887889724,\n",
       " 'birth_date': None,\n",
       " 'summary': None,\n",
       " 'location': {'country': 'Netherlands',\n",
       "  'short': 'Amsterdam, North Holland',\n",
       "  'city': 'Amsterdam',\n",
       "  'state': 'North Holland',\n",
       "  'default': 'Amsterdam, North Holland, Netherlands'},\n",
       " 'premium': False,\n",
       " 'influencer': False,\n",
       " 'treasury_media': [],\n",
       " 'languages': {'primary_locale': {'country': 'US', 'language': 'en'},\n",
       "  'supported_locales': [{'country': 'US', 'language': 'en'}],\n",
       "  'profile_languages': []},\n",
       " 'industry': 'Sports',\n",
       " 'education': [],\n",
       " 'patents': [],\n",
       " 'awards': [],\n",
       " 'certifications': [],\n",
       " 'organizations': [],\n",
       " 'projects': [],\n",
       " 'publications': [],\n",
       " 'courses': [],\n",
       " 'test_scores': [],\n",
       " 'position_groups': [{'company': {'id': 549880,\n",
       "    'name': 'RKC Waalwijk',\n",
       "    'logo': 'https://media.licdn.com/dms/image/C4D0BAQEC_cbNl3HLhg/company-logo_400_400/0/1531133812048?e=1704931200&v=beta&t=A6a7LjDR5mBJT5MtV3JcSewqRe9vTqWOLk5C0uZYGRs',\n",
       "    'url': 'https://www.linkedin.com/company/rkc-waalwijk/',\n",
       "    'employees': {'start': 11, 'end': 50}},\n",
       "   'date': {'start': {'month': 7, 'day': None, 'year': 2020},\n",
       "    'end': {'month': None, 'day': None, 'year': None}},\n",
       "   'profile_positions': [{'location': None,\n",
       "     'date': {'start': {'month': 7, 'day': None, 'year': 2020},\n",
       "      'end': {'month': None, 'day': None, 'year': None}},\n",
       "     'company': 'RKC Waalwijk',\n",
       "     'description': None,\n",
       "     'title': 'Soccer Player',\n",
       "     'employment_type': 'Full-time'}]}],\n",
       " 'volunteer_experiences': [],\n",
       " 'skills': [],\n",
       " 'network_info': None,\n",
       " 'related_profiles': None,\n",
       " 'contact_info': None}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Motivator', 'Harmonizer', 'Harmonizer', 'Initiator', 'Harmonizer',\n",
       "       'Counselor', 'Harmonizer', 'Harmonizer', 'Motivator', 'Influencer',\n",
       "       'Harmonizer', 'Encourager', 'Motivator', 'Motivator', 'Harmonizer',\n",
       "       'Initiator', 'Motivator', 'Harmonizer', 'Harmonizer', 'Motivator'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform(predicted_character)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
