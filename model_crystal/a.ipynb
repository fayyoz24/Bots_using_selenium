{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/fayyoz24/Bots_using_selenium/main/model_crystal/without%20certifications%20linkedin%20profiles.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>characters</th>\n",
       "      <th>name</th>\n",
       "      <th>position 1</th>\n",
       "      <th>position 2</th>\n",
       "      <th>experince 1</th>\n",
       "      <th>experince 2</th>\n",
       "      <th>field of studies 1</th>\n",
       "      <th>field of studies 2</th>\n",
       "      <th>degree 1</th>\n",
       "      <th>degree 2</th>\n",
       "      <th>industry</th>\n",
       "      <th>skills</th>\n",
       "      <th>influencer</th>\n",
       "      <th>country</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://nl.linkedin.com/in/mieke-zonneveld-58a...</td>\n",
       "      <td>Encourager (Is)</td>\n",
       "      <td>Mieke Zonneveld</td>\n",
       "      <td>Docent Nederlands</td>\n",
       "      <td>Auteur</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Opleiding leraar Nederlands in de eerste graad</td>\n",
       "      <td>Nederlandse taal- en letterkunde</td>\n",
       "      <td>Master</td>\n",
       "      <td>Master of Arts - MA</td>\n",
       "      <td>Primary/Secondary Education</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://nl.linkedin.com/in/jiska-de-ligt-2a7527b</td>\n",
       "      <td>Counselor (Si)</td>\n",
       "      <td>Jiska de Ligt</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>bestuurslid (secretaris)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Comparative Literature</td>\n",
       "      <td>English Language and Literature/Letters</td>\n",
       "      <td>Research master</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Information Technology &amp; Services</td>\n",
       "      <td>['Academic Writing', 'English Literature', 'Li...</td>\n",
       "      <td>False</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Bij Infi ben ik verantwoordelijk voor het verz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               links       characters  \\\n",
       "0  https://nl.linkedin.com/in/mieke-zonneveld-58a...  Encourager (Is)   \n",
       "1   https://nl.linkedin.com/in/jiska-de-ligt-2a7527b   Counselor (Si)   \n",
       "\n",
       "              name         position 1                position 2  experince 1  \\\n",
       "0  Mieke Zonneveld  Docent Nederlands                    Auteur          9.0   \n",
       "1    Jiska de Ligt         HR Manager  bestuurslid (secretaris)          1.0   \n",
       "\n",
       "   experince 2                              field of studies 1  \\\n",
       "0          9.0  Opleiding leraar Nederlands in de eerste graad   \n",
       "1          1.0                          Comparative Literature   \n",
       "\n",
       "                        field of studies 2         degree 1  \\\n",
       "0         Nederlandse taal- en letterkunde           Master   \n",
       "1  English Language and Literature/Letters  Research master   \n",
       "\n",
       "              degree 2                           industry  \\\n",
       "0  Master of Arts - MA        Primary/Secondary Education   \n",
       "1                  NaN  Information Technology & Services   \n",
       "\n",
       "                                              skills influencer      country  \\\n",
       "0                                                 []      False  Netherlands   \n",
       "1  ['Academic Writing', 'English Literature', 'Li...      False  Netherlands   \n",
       "\n",
       "                                             summary  \n",
       "0                                                NaN  \n",
       "1  Bij Infi ben ik verantwoordelijk voor het verz...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(\"Unnamed: 0\", axis=1).head()[:2][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/fayyoz24/Bots_using_selenium/main/model_crystal/without%20certifications%20linkedin%20profiles.csv')\n",
    "\n",
    "# Convert non-string values to strings in the feature columns\n",
    "text_features = data[['position 1', 'position 2', 'field of studies 1', 'field of studies 2', 'degree 1', 'degree 2', 'industry', 'skills', 'influencer', 'country']].copy()\n",
    "\n",
    "# Handle non-string values in each column\n",
    "for column in text_features.columns:\n",
    "    text_features[column] = text_features[column].astype(str)\n",
    "\n",
    "# Combine all text features into a single string column\n",
    "text_data = text_features.apply(lambda x: ' '.join(x), axis=1).tolist()\n",
    "labels = data['characters'].tolist()\n",
    "\n",
    "# Convert labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(set(labels))}\n",
    "y = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Update label mapping to start from 0\n",
    "label_mapping = {label: idx for label, idx in label_mapping.items()}\n",
    "num_classes = len(label_mapping)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to have consistent length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "print(f\"max_sequence------>>>>{max_sequence_length}\")\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert the data to NumPy arrays\n",
    "X_text = np.array(padded_sequences)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Textual input branch\n",
    "text_input = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(vocab_size, 100, input_length=max_sequence_length)(text_input)\n",
    "lstm_layer = LSTM(100)(embedding_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(lstm_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=text_input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_text, y_train, validation_data=(X_test_text, y_test),\n",
    "          epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model\n",
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 65s 527ms/step - loss: 2.5251 - accuracy: 0.1548 - val_loss: 2.4070 - val_accuracy: 0.1966\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 59s 513ms/step - loss: 2.2792 - accuracy: 0.2105 - val_loss: 2.3431 - val_accuracy: 0.2015\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 59s 513ms/step - loss: 2.0298 - accuracy: 0.2768 - val_loss: 2.4085 - val_accuracy: 0.2064\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 59s 512ms/step - loss: 1.7504 - accuracy: 0.3808 - val_loss: 2.4702 - val_accuracy: 0.1941\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 59s 509ms/step - loss: 1.3977 - accuracy: 0.5242 - val_loss: 2.5675 - val_accuracy: 0.2088\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 59s 511ms/step - loss: 1.0272 - accuracy: 0.6631 - val_loss: 2.7992 - val_accuracy: 0.2260\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 58s 507ms/step - loss: 0.7348 - accuracy: 0.7767 - val_loss: 3.1143 - val_accuracy: 0.2285\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 58s 508ms/step - loss: 0.5412 - accuracy: 0.8471 - val_loss: 3.2208 - val_accuracy: 0.2187\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 60s 515ms/step - loss: 0.4144 - accuracy: 0.8804 - val_loss: 3.4457 - val_accuracy: 0.2187\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 59s 510ms/step - loss: 0.3376 - accuracy: 0.9031 - val_loss: 3.7078 - val_accuracy: 0.2211\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 58s 507ms/step - loss: 0.2572 - accuracy: 0.9301 - val_loss: 4.1102 - val_accuracy: 0.1916\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 58s 507ms/step - loss: 0.2207 - accuracy: 0.9356 - val_loss: 4.1331 - val_accuracy: 0.2187\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 58s 506ms/step - loss: 0.2134 - accuracy: 0.9383 - val_loss: 4.1473 - val_accuracy: 0.2359\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 58s 508ms/step - loss: 0.1819 - accuracy: 0.9476 - val_loss: 4.1608 - val_accuracy: 0.2408\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 60s 519ms/step - loss: 0.1632 - accuracy: 0.9500 - val_loss: 4.5138 - val_accuracy: 0.2432\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 58s 507ms/step - loss: 0.1477 - accuracy: 0.9571 - val_loss: 4.4326 - val_accuracy: 0.2334\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 58s 507ms/step - loss: 0.1378 - accuracy: 0.9599 - val_loss: 4.6465 - val_accuracy: 0.2285\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 59s 514ms/step - loss: 0.1387 - accuracy: 0.9588 - val_loss: 4.6383 - val_accuracy: 0.2187\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 57s 498ms/step - loss: 0.1378 - accuracy: 0.9585 - val_loss: 4.6443 - val_accuracy: 0.2064\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 62s 541ms/step - loss: 0.1280 - accuracy: 0.9599 - val_loss: 4.8260 - val_accuracy: 0.2015\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 60s 524ms/step - loss: 0.1345 - accuracy: 0.9588 - val_loss: 4.9234 - val_accuracy: 0.2064\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 58s 502ms/step - loss: 0.1490 - accuracy: 0.9536 - val_loss: 4.6504 - val_accuracy: 0.2260\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 59s 509ms/step - loss: 0.1319 - accuracy: 0.9574 - val_loss: 4.8120 - val_accuracy: 0.2088\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 58s 506ms/step - loss: 0.1323 - accuracy: 0.9601 - val_loss: 4.7888 - val_accuracy: 0.2162\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 58s 505ms/step - loss: 0.1287 - accuracy: 0.9604 - val_loss: 4.7778 - val_accuracy: 0.2285\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - 58s 504ms/step - loss: 0.1261 - accuracy: 0.9610 - val_loss: 4.9441 - val_accuracy: 0.2138\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - 59s 509ms/step - loss: 0.1288 - accuracy: 0.9593 - val_loss: 4.9140 - val_accuracy: 0.2162\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - 58s 501ms/step - loss: 0.1141 - accuracy: 0.9629 - val_loss: 5.0898 - val_accuracy: 0.2138\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - 59s 514ms/step - loss: 0.1384 - accuracy: 0.9577 - val_loss: 4.6424 - val_accuracy: 0.2113\n",
      "Epoch 30/50\n",
      "115/115 [==============================] - 58s 509ms/step - loss: 0.1213 - accuracy: 0.9618 - val_loss: 4.8716 - val_accuracy: 0.2015\n",
      "Epoch 31/50\n",
      "115/115 [==============================] - 58s 504ms/step - loss: 0.1204 - accuracy: 0.9615 - val_loss: 4.9118 - val_accuracy: 0.2064\n",
      "Epoch 32/50\n",
      "115/115 [==============================] - 58s 502ms/step - loss: 0.1210 - accuracy: 0.9621 - val_loss: 4.9544 - val_accuracy: 0.2015\n",
      "Epoch 33/50\n",
      "115/115 [==============================] - 58s 505ms/step - loss: 0.1091 - accuracy: 0.9648 - val_loss: 5.0815 - val_accuracy: 0.2113\n",
      "Epoch 34/50\n",
      "115/115 [==============================] - 64s 554ms/step - loss: 0.1064 - accuracy: 0.9640 - val_loss: 5.1497 - val_accuracy: 0.2260\n",
      "Epoch 35/50\n",
      "115/115 [==============================] - 62s 539ms/step - loss: 0.1067 - accuracy: 0.9667 - val_loss: 5.1895 - val_accuracy: 0.2334\n",
      "Epoch 36/50\n",
      "115/115 [==============================] - 58s 503ms/step - loss: 0.1060 - accuracy: 0.9659 - val_loss: 5.1938 - val_accuracy: 0.2211\n",
      "Epoch 37/50\n",
      "115/115 [==============================] - 62s 536ms/step - loss: 0.1063 - accuracy: 0.9645 - val_loss: 5.2474 - val_accuracy: 0.2359\n",
      "Epoch 38/50\n",
      "115/115 [==============================] - 58s 508ms/step - loss: 0.1057 - accuracy: 0.9661 - val_loss: 5.2343 - val_accuracy: 0.2334\n",
      "Epoch 39/50\n",
      "115/115 [==============================] - 65s 566ms/step - loss: 0.1060 - accuracy: 0.9672 - val_loss: 5.2889 - val_accuracy: 0.2383\n",
      "Epoch 40/50\n",
      "115/115 [==============================] - 58s 505ms/step - loss: 0.1046 - accuracy: 0.9659 - val_loss: 5.3572 - val_accuracy: 0.2334\n",
      "Epoch 41/50\n",
      "115/115 [==============================] - 62s 537ms/step - loss: 0.1061 - accuracy: 0.9634 - val_loss: 5.3471 - val_accuracy: 0.2383\n",
      "Epoch 42/50\n",
      "115/115 [==============================] - 66s 576ms/step - loss: 0.1060 - accuracy: 0.9661 - val_loss: 5.3704 - val_accuracy: 0.2408\n",
      "Epoch 43/50\n",
      "115/115 [==============================] - 65s 564ms/step - loss: 0.1054 - accuracy: 0.9645 - val_loss: 5.4014 - val_accuracy: 0.2285\n",
      "Epoch 44/50\n",
      "115/115 [==============================] - 59s 509ms/step - loss: 0.1055 - accuracy: 0.9651 - val_loss: 5.4398 - val_accuracy: 0.2383\n",
      "Epoch 45/50\n",
      "115/115 [==============================] - 60s 525ms/step - loss: 0.1058 - accuracy: 0.9651 - val_loss: 5.4551 - val_accuracy: 0.2383\n",
      "Epoch 46/50\n",
      "115/115 [==============================] - 60s 524ms/step - loss: 0.1055 - accuracy: 0.9659 - val_loss: 5.4364 - val_accuracy: 0.2408\n",
      "Epoch 47/50\n",
      "115/115 [==============================] - 68s 596ms/step - loss: 0.1053 - accuracy: 0.9640 - val_loss: 5.4861 - val_accuracy: 0.2457\n",
      "Epoch 48/50\n",
      "115/115 [==============================] - 63s 550ms/step - loss: 0.1054 - accuracy: 0.9642 - val_loss: 5.5252 - val_accuracy: 0.2310\n",
      "Epoch 49/50\n",
      "115/115 [==============================] - 68s 588ms/step - loss: 0.1046 - accuracy: 0.9648 - val_loss: 5.5403 - val_accuracy: 0.2359\n",
      "Epoch 50/50\n",
      "115/115 [==============================] - 58s 507ms/step - loss: 0.1046 - accuracy: 0.9659 - val_loss: 5.5690 - val_accuracy: 0.2408\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/fayyoz24/Bots_using_selenium/main/model_crystal/without%20certifications%20linkedin%20profiles.csv')\n",
    "cols=['position 1', 'position 2', 'field of studies 1','experince 1', 'experince 2', 'field of studies 2',\n",
    "       'degree 1', 'degree 2', 'industry', 'skills', 'influencer', 'country']\n",
    "\n",
    "for col in cols:\n",
    "  data[col] = data[col].fillna(\"Unknown\")\n",
    "# Convert non-string values to strings in the feature columns\n",
    "text_features = data[['position 1', 'position 2', \"experince 1\",\"experince 2\", \n",
    "                      'field of studies 1', 'field of studies 2', \n",
    "                      'degree 1', 'degree 2', 'industry', 'skills',\n",
    "                      'influencer', 'country', 'summary']].copy()\n",
    "\n",
    "# Handle non-string values in each column\n",
    "for column in text_features.columns:\n",
    "    text_features[column] = text_features[column].astype(str)\n",
    "\n",
    "# Combine all text features into a single string column\n",
    "text_data = text_features.apply(lambda x: ' '.join(x), axis=1).tolist()\n",
    "labels = data['characters'].tolist()\n",
    "\n",
    "# Convert labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(set(labels))}\n",
    "y = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Update label mapping to start from 0\n",
    "label_mapping = {label: idx for label, idx in label_mapping.items()}\n",
    "num_classes = len(label_mapping)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to have consistent length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "print(max_sequence_length)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert the data to NumPy arrays\n",
    "X_text = np.array(padded_sequences)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Textual input branch\n",
    "text_input = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(vocab_size, 100, input_length=max_sequence_length)(text_input)\n",
    "lstm_layer = LSTM(100)(embedding_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(lstm_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=text_input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_text, y_train, validation_data=(X_test_text, y_test),\n",
    "          epochs=50, batch_size=32)\n",
    "\n",
    "# Save the model\n",
    "model.save('model2_test_size=0.1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sequence------>>>>519\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Text: Frontend mentor \tPredicted Label: Supporter (S)\n",
      "Text: Fullstack | javaScript developer\tPredicted Label: Analyst (C)\n",
      "Text: 0\tPredicted Label: Supporter (S)\n",
      "Text: Petroleum Engineering\tPredicted Label: Analyst (C)\n",
      "Text: Oil and Gas\tPredicted Label: Captain (D)\n",
      "Text: Master of Science - MS\tPredicted Label: Analyst (C)\n",
      "Text: Bachelor's degree\tPredicted Label: Editor (Cs)\n",
      "Text: Computer Software\tPredicted Label: Supporter (S)\n",
      "Text: ['Python (Programming Language)', 'Django', 'REST APIs', 'Problem Solving', 'Responsive Web Design', 'Front-End Development', 'Firebase', 'Web Development', 'Communication', 'Contract Negotiation', 'Construction', 'Management', 'Analytical Skills', 'Software as a Service (SaaS)', 'Translation', 'Technical Translation', 'English Translation', 'Russian Translation', 'Redux.js', 'Git']\tPredicted Label: Analyst (C)\n",
      "Text: I am a skilled Frontend Developer with over 1.5 years of experience and a proven track record of delivering high-quality work within tight deadlines. My expertise includes using a range of technologies such as HTML5, CSS3, JavaScript, Reactjs, Redux, Redux Toolkit, MaterialUI, TailwindCSS, Materialize UI, and Bootstrap. I am committed to enhancing my skills and staying up-to-date with the latest advancements in the field. I am a strong team player and effectively collaborate with other developers, designers, and project managers. My unique skill set also includes the ability to translate technical concepts to non-technical stakeholders, making me an effective liaison between management and the development team.\tPredicted Label: Counselor (Si)\n",
      "Analyst (C)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "positions0=[]\n",
    "positions1=[]\n",
    "experinces0=[]\n",
    "experinces1=[]\n",
    "skills=[]\n",
    "influencer=[]\n",
    "country=[]\n",
    "field_of_studies0=[]\n",
    "field_of_studies1=[]\n",
    "degrees0=[]\n",
    "degrees1=[]\n",
    "industries=[]\n",
    "names=[]\n",
    "summaries=[]\n",
    "certifications=[]\n",
    "\n",
    "link=\"https://www.linkedin.com/in/ismatulla-kuyliev-43286a199/\"\n",
    "id = link.split('/')[4]\n",
    "url = \"https://api.iscraper.io/v2/profile-details\"\n",
    "\n",
    "payload = {\n",
    "  'profile_id': id,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  'X-API-KEY': 'hVSqiv11cY1W5YUawXUDLBn0jb4G5W44',\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "res=response.json()\n",
    "try:\n",
    "  skills.append(res['skills'])\n",
    "except:\n",
    "  skills.append(\"Unknown\")\n",
    "position0=\"\"\n",
    "try:\n",
    "  position0=res['position_groups'][0]['profile_positions'][0]['title']\n",
    "  positions0.append(position0)\n",
    "except:\n",
    "  positions0.append(\"Unknown\")\n",
    "\n",
    "position1=\"\"\n",
    "try:\n",
    "  position1=res['position_groups'][1]['profile_positions'][0]['title']\n",
    "  positions1.append(position1)\n",
    "except:\n",
    "  positions1.append(\"Unknown\")\n",
    "\n",
    "experince0=0\n",
    "try:\n",
    "  experince0=res['position_groups'][0]['date']['start']['year']-res['position_groups'][0]['date']['end']['year']\n",
    "  experinces0.append(experince0)\n",
    "except:\n",
    "  try:\n",
    "    experince0=datetime.date.today().year - res['position_groups'][0]['date']['start']['year']\n",
    "    experinces0.append(experince0)\n",
    "  except:\n",
    "    experinces0.append(\"Unknown\")\n",
    "\n",
    "experince1=0\n",
    "try:\n",
    "  experince1=res['position_groups'][1]['date']['start']['year']-res['position_groups'][1]['date']['end']['year']\n",
    "  experinces1.append(experince1.abs())\n",
    "except:\n",
    "  try:\n",
    "    experince1=datetime.date.today().year - res['position_groups'][1]['date']['start']['year']\n",
    "    experinces1.append(experince1.abs())\n",
    "  except:\n",
    "    experinces1.append(\"Unknown\")\n",
    "      \n",
    "try:\n",
    "  influencer.append(res['influencer'])\n",
    "except:\n",
    "  influencer.append(\"Unknown\")\n",
    "try:\n",
    "  country.append(res['location']['country'])\n",
    "except:\n",
    "  country.append(\"Unknown\")\n",
    "field_of_study0=\"\"\n",
    "try:\n",
    "  field_of_study0=res['education'][0]['field_of_study']\n",
    "  field_of_studies0.append(field_of_study0)\n",
    "except:\n",
    "  field_of_study0=\"Unknown\"\n",
    "  field_of_studies0.append(field_of_study0)\n",
    "\n",
    "field_of_study1=\"\"\n",
    "try:\n",
    "  field_of_study1=res['education'][1]['field_of_study']\n",
    "  field_of_studies1.append(field_of_study1)\n",
    "except:\n",
    "  field_of_study1=\"Unknown\"\n",
    "  field_of_studies1.append(field_of_study1)\n",
    "\n",
    "degree_name0=''\n",
    "try:\n",
    "  degree_name0=res['education'][0]['degree_name']\n",
    "  degrees0.append(degree_name0)\n",
    "except:\n",
    "  degree_name0=\"Unknown\"\n",
    "  degrees0.append(degree_name0)\n",
    "\n",
    "degree_name1=''\n",
    "try:\n",
    "  degree_name1=res['education'][1]['degree_name']\n",
    "  degrees1.append(degree_name1)\n",
    "except:\n",
    "  degree_name1=\"Unknown\"\n",
    "  degrees1.append(degree_name1)\n",
    "\n",
    "try:\n",
    "  industries.append(res['industry'])\n",
    "except:\n",
    "  industries.append(\"Unknown\")\n",
    "\n",
    "try:\n",
    "  if res['summary']:\n",
    "    summaries.append(res['summary'])\n",
    "  else:\n",
    "    summaries.append(\"Unknown\")\n",
    "except:\n",
    "  summaries.append(\"Unknown\")\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('model.h5')\n",
    "a= [\n",
    "    positions0[0],\n",
    "    positions1[0],\n",
    "    str(experinces0[0]),\n",
    "    str(experinces1[0]),\n",
    "    field_of_studies0[0],\n",
    "    field_of_studies1[0],\n",
    "    degrees0[0],\n",
    "    degrees1[0],\n",
    "    industries[0],\n",
    "    skills[0],\n",
    "    summaries[0]\n",
    "]\n",
    "new_text_data=[]\n",
    "for i in a:\n",
    "    if i != \"Unknown\":\n",
    "        new_text_data.append(i)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(new_text_data)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(new_text_data)\n",
    "\n",
    "# Pad sequences to have consistent length\n",
    "max_sequence_length = 519\n",
    "print(f\"max_sequence------>>>>{max_sequence_length}\")\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "# Preprocess the new text data\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text_data)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the predicted labels back to original labels\n",
    "label_mapping={'Analyst (C)': 0, 'Questioner (CD)': 1, 'Captain (D)': 2, 'Encourager (Is)': 3, 'Stabilizer (SC)': 4, \n",
    "  'Supporter (S)': 5, 'Counselor (Si)': 6, 'Editor (Cs)': 7, 'Motivator (I)': 8, 'Driver (Di)': 9,\n",
    "  'Planner (Sc)': 10, 'Skeptic (Cd)': 11, 'Architect (Dc)': 12, 'Driver (Di);': 13, \n",
    "  'Influencer (Id)': 14, 'Harmonizer (IS)': 15, 'Initiator (DI)': 16}\n",
    "reverse_label_mapping = {idx: label for label, idx in label_mapping.items()}\n",
    "predicted_labels = [reverse_label_mapping[label] for label in predicted_labels]\n",
    "\n",
    "# Print the predictions\n",
    "for text, label in zip(new_text_data, predicted_labels):\n",
    "    print(f\"Text: {text}\\tPredicted Label: {label}\")\n",
    "print(max(set(predicted_labels), key = predicted_labels.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Supporter (S)',\n",
       " 'Motivator (I)',\n",
       " 'Harmonizer (IS)',\n",
       " 'Questioner (CD)',\n",
       " 'Counselor (Si)',\n",
       " 'Supporter (S)',\n",
       " 'Harmonizer (IS)',\n",
       " 'Supporter (S)',\n",
       " 'Harmonizer (IS)',\n",
       " 'Influencer (Id)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_text_data = [\n",
    "#     positions0[0],\n",
    "#     positions1[0],\n",
    "#     str(experinces0[0]),\n",
    "#     str(experinces1[0]),\n",
    "#     field_of_studies0[0],\n",
    "#     field_of_studies1[0],\n",
    "#     degrees0[0],\n",
    "#     degrees1[0],\n",
    "#     industries[0],\n",
    "#     skills[0],\n",
    "#     # str(influencer[0]),\n",
    "#     # country[0],\n",
    "#     summaries[0]\n",
    "\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Text: Sustainability Partner to Boards - Director Marketing & Sales\tPredicted Label: Encourager (Is)\n",
      "Text: Founder\tPredicted Label: Captain (D)\n",
      "Text: 14\tPredicted Label: Driver (Di);\n",
      "Text: Marketing\tPredicted Label: Motivator (I)\n",
      "Text: Post-doctorate degree in Brand Management\tPredicted Label: Architect (Dc)\n",
      "Text: Environmental Services\tPredicted Label: Captain (D)\n",
      "Text: ['Leadership Development', 'Investments', 'Strategy', 'Leadership', 'Sustainability Consulting', 'Marketing Strategy', 'Business Strategy', 'Marketing', 'B2B', 'New Business Development', 'Business Development', 'Marketing Management', 'Entrepreneurship', 'Start-ups', 'Project Planning', 'Supply Chain', 'Management Consulting', 'Change Management', 'FMCG', 'Strategic Planning']\tPredicted Label: Motivator (I)\n",
      "Text: Committed to help leaders at all levels transition their organisations towards businesses that are a force for good... leaving no footprint.\n",
      "\n",
      "Main fields of expertise: Strategy, Transformation, Marketing & Commerce, Sustainable Businesses, Leadership, B Corp & Dialogue.\tPredicted Label: Motivator (I)\n",
      "Motivator (I)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('model.h5')\n",
    "a= [\n",
    "    positions0[0],\n",
    "    positions1[0],\n",
    "    str(experinces0[0]),\n",
    "    str(experinces1[0]),\n",
    "    field_of_studies0[0],\n",
    "    field_of_studies1[0],\n",
    "    degrees0[0],\n",
    "    degrees1[0],\n",
    "    industries[0],\n",
    "    skills[0],\n",
    "    summaries[0]\n",
    "]\n",
    "new_text_data=[]\n",
    "for i in a:\n",
    "    if i != \"Unknown\" and i != None:\n",
    "        new_text_data.append(i)\n",
    "\n",
    "# Preprocess the new text data\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text_data)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the predicted labels back to original labels\n",
    "reverse_label_mapping = {idx: label for label, idx in label_mapping.items()}\n",
    "predicted_labels = [reverse_label_mapping[label] for label in predicted_labels]\n",
    "\n",
    "# Print the predictions\n",
    "for text, label in zip(new_text_data, predicted_labels):\n",
    "    print(f\"Text: {text}\\tPredicted Label: {label}\")\n",
    "print(max(set(predicted_labels), key = predicted_labels.count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Editor (Cs)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(set(predicted_labels), key = predicted_labels.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Editor (Cs)',\n",
       " 'Supporter (S)',\n",
       " 'Supporter (S)',\n",
       " 'Editor (Cs)',\n",
       " 'Editor (Cs)',\n",
       " 'Captain (D)',\n",
       " 'Editor (Cs)']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positions0[0]\n",
    "# positions1[0]\n",
    "# str(experinces0[0])\n",
    "# str(experinces1[0])\n",
    "# field_of_studies0[0]\n",
    "# field_of_studies1[0]\n",
    "# degrees0[0]\n",
    "# degrees1[0]\n",
    "# industries[0]\n",
    "# skills[0]\n",
    "# str(influencer[0])\n",
    "# country[0]\n",
    "summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[0;32m     72\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m  \u001b[39m# Decrease the learning rate\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlearning_rate),\n\u001b[0;32m     75\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     76\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     78\u001b[0m \u001b[39m# Apply early stopping to prevent overfitting\u001b[39;00m\n\u001b[0;32m     79\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/fayyoz24/Bots_using_selenium/main/model_crystal/without%20certifications%20linkedin%20profiles.csv')\n",
    "cols = ['position 1', 'position 2', 'field of studies 1', 'experince 1', 'experince 2', 'field of studies 2',\n",
    "        'degree 1', 'degree 2', 'industry', 'skills', 'influencer', 'country']\n",
    "\n",
    "for col in cols:\n",
    "    data[col] = data[col].fillna(\"Unknown\")\n",
    "\n",
    "# Convert non-string values to strings in the feature columns\n",
    "text_features = data[\n",
    "    ['position 1', 'position 2', \"experince 1\", \"experince 2\", 'field of studies 1', 'field of studies 2',\n",
    "     'degree 1', 'degree 2', 'industry', 'skills', 'influencer', 'country', 'summary']].copy()\n",
    "\n",
    "# Handle non-string values in each column\n",
    "for column in text_features.columns:\n",
    "    text_features[column] = text_features[column].astype(str)\n",
    "\n",
    "# Combine all text features into a single string column\n",
    "text_data = text_features.apply(lambda x: ' '.join(x), axis=1).tolist()\n",
    "labels = data['characters'].tolist()\n",
    "\n",
    "# Convert labels to integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(set(labels))}\n",
    "y = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Update label mapping to start from 0\n",
    "label_mapping = {label: idx for label, idx in label_mapping.items()}\n",
    "num_classes = len(label_mapping)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to have consistent length\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "print(max_sequence_length)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert the data to NumPy arrays\n",
    "X_text = np.array(padded_sequences)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Textual input branch\n",
    "text_input = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(vocab_size, 100, input_length=max_sequence_length)(text_input)\n",
    "lstm_layer = LSTM(100, kernel_regularizer=l2(0.01))(embedding_layer)\n",
    "dropout_layer = Dropout(0.5)(lstm_layer)  # Add a dropout layer for regularization\n",
    "output_layer = Dense(num_classes, activation='softmax')(dropout_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=text_input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "learning_rate = 0.001  # Decrease the learning\n",
    "# Compile the model\n",
    "learning_rate = 0.001  # Decrease the learning rate\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Apply early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_text, y_train,\n",
    "                    validation_data=(X_test_text, y_test),\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('model3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1MAbPt8f4U14J6XsT5WHxrTdgo1VpPAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5jcbZqwAKUU6MFDxRnMAfFUf6p18KT9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5001\n",
      "5002\n",
      "5003\n",
      "5004\n",
      "5005\n",
      "5006\n",
      "5007\n",
      "5008\n",
      "5009\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "data=pd.read_csv(\"./partly_45878_reach_50.csv\")\n",
    "for i in range(5000, 5010):\n",
    "  link=data['links'][i]\n",
    "  id = link.split('/')[4]\n",
    "  url = \"https://api.iscraper.io/v2/profile-details\"\n",
    "\n",
    "  payload = {\n",
    "    'profile_id': id,\n",
    "  }\n",
    "\n",
    "  headers = {\n",
    "    'X-API-KEY': '1MAbPt8f4U14J6XsT5WHxrTdgo1VpPAF',\n",
    "  }\n",
    "\n",
    "  response = requests.post(url, json=payload, headers=headers)\n",
    "  res=response.json()\n",
    "\n",
    "  with open(f\"./datas/json_responses_50k/{i}.json\", \"w\") as json_file:\n",
    "      json.dump(res, json_file, indent=4)\n",
    "  print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
